# Project Structure

## ğŸ“ Complete Project Organization

```
Sentiment Based Anlysis/
â”‚
â”œâ”€â”€ ğŸ“Š data/                          # Data directory
â”‚   â””â”€â”€ stock_tweets.csv              # Main dataset (moved from root)
â”‚
â”œâ”€â”€ ğŸ§  src/                           # Source code
â”‚   â”œâ”€â”€ __init__.py                   # Package initialization
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/                # Text preprocessing modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ text_preprocessor.py     # Tokenization, stopwords, lemmatization
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Sentiment analysis models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ sentiment_analyzer.py    # VADER, TextBlob, BERT implementations
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/                   # Model evaluation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py               # Accuracy, Precision, Recall, F1, CM
â”‚   â”‚
â”‚   â””â”€â”€ utils/                        # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ data_loader.py            # Data loading and preparation
â”‚
â”œâ”€â”€ ğŸ¯ models/                        # Output directory for trained models/results
â”‚   â”œâ”€â”€ sentiment_results.csv        # Generated after training
â”‚   â”œâ”€â”€ summary_statistics.csv       # Generated after training
â”‚   â””â”€â”€ stock_sentiment.csv          # Generated after training
â”‚
â”œâ”€â”€ ğŸ““ notebooks/                     # Jupyter notebooks
â”‚   â””â”€â”€ exploratory_analysis.ipynb    # EDA notebook
â”‚
â”œâ”€â”€ ğŸŒ app/                           # Streamlit web application
â”‚   â””â”€â”€ app.py                        # Main Streamlit app
â”‚
â”œâ”€â”€ âš™ï¸ config/                        # Configuration files
â”‚   â””â”€â”€ config.yaml                   # Project configuration
â”‚
â”œâ”€â”€ ğŸš€ train.py                       # Training script
â”œâ”€â”€ ğŸ” inference.py                   # Inference script for single texts
â”œâ”€â”€ ğŸ“¦ setup.py                       # Package setup script
â”œâ”€â”€ ğŸ“‹ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ“– README.md                      # Main documentation
â”œâ”€â”€ ğŸƒ QUICKSTART.md                  # Quick start guide
â”œâ”€â”€ ğŸ“ PROJECT_STRUCTURE.md           # This file
â””â”€â”€ .gitignore                        # Git ignore rules
```

## ğŸ¯ Key Components

### 1. Preprocessing (`src/preprocessing/`)
- **TextPreprocessor**: Complete NLP preprocessing pipeline
  - Normalization (lowercase, URL removal, HTML entities)
  - Tokenization (TweetTokenizer for social media)
  - Stopword removal
  - Lemmatization with POS tagging

### 2. Models (`src/models/`)
- **SentimentAnalyzer**: Unified sentiment analysis interface
  - VADER: Rule-based, optimized for social media
  - TextBlob: Simple polarity and subjectivity
  - BERT: RoBERTa-based transformer model
  - Ensemble: Combines all models

### 3. Evaluation (`src/evaluation/`)
- **ModelEvaluator**: Comprehensive evaluation metrics
  - Accuracy, Precision, Recall, F1-Score
  - Per-class metrics
  - Confusion matrix visualization
  - Model comparison

### 4. Application (`app/`)
- **Streamlit App**: Interactive web interface
  - Dataset overview
  - Single text analysis
  - Batch processing
  - Trend visualization
  - Results export

### 5. Scripts
- **train.py**: Full pipeline training script
- **inference.py**: Quick sentiment analysis for single texts

## ğŸ“Š Data Flow

```
CSV Data â†’ Data Loader â†’ Preprocessing â†’ Sentiment Analysis â†’ Evaluation â†’ Results
```

## ğŸ”§ Technology Stack

- **Core**: Python 3.8+, Pandas, NumPy
- **NLP**: NLTK, VADER, TextBlob
- **ML/DL**: Scikit-learn, PyTorch, Transformers
- **Visualization**: Matplotlib, Seaborn, Plotly
- **Web**: Streamlit
- **Notebooks**: Jupyter

## ğŸ“ˆ Output Files

After running `train.py`:
1. `models/sentiment_results.csv` - Full results with predictions
2. `models/summary_statistics.csv` - Overall statistics
3. `models/stock_sentiment.csv` - Stock-wise analysis

## ğŸ“ Usage Patterns

1. **Quick Analysis**: `python inference.py --text "Your text here"`
2. **Full Training**: `python train.py --preprocess --use_bert`
3. **Web App**: `streamlit run app/app.py`
4. **Exploration**: Open `notebooks/exploratory_analysis.ipynb`

## âœ… Project Checklist

- [x] Proper folder structure
- [x] Text preprocessing module
- [x] Multiple sentiment models (VADER, TextBlob, BERT)
- [x] Evaluation metrics (Accuracy, Precision, Recall, F1, CM)
- [x] Training script
- [x] Inference script
- [x] Web application (Streamlit)
- [x] Configuration files
- [x] Documentation (README, Quick Start)
- [x] Requirements file
- [x] Jupyter notebook for exploration
- [x] Git ignore file
- [x] Setup script

## ğŸš€ Next Steps

1. Install dependencies: `pip install -r requirements.txt`
2. Run quick test: `python train.py --sample_size 500`
3. Launch app: `streamlit run app/app.py`
4. Explore notebook: Open `notebooks/exploratory_analysis.ipynb`
